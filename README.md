# **MediSeek: Fine-Tuning DeepSeek for Medical AI Chatbots**  

**MediSeek** is a project focused on fine-tuning **DeepSeek-R1-Distill-Qwen-14B**, an open-source large language model (LLM), to enhance its capabilities for **medical chatbot applications**. By leveraging **supervised fine-tuning (SFT)** and **Parameter-Efficient Fine-Tuning (PEFT)**, this project improves the model’s ability to handle medical terminology, provide expert-level responses, and support healthcare decision-making.  

## **Why Fine-Tune for Medical AI?**  
Fine-tuning ensures the model can:  
✔ **Accurately interpret medical terminology** and domain-specific concepts.  
✔ **Provide structured, empathetic, and reliable responses** to medical queries.  
✔ **Adhere to healthcare industry standards** for safety and compliance.  
✔ **Enhance reasoning transparency** using **Chain of Thought (CoT) reasoning**.  
✔ **Specialize in key medical applications** such as **diagnosis, symptom assessment, treatment guidance, and literature summarization**.  

## **Fine-Tuning Process**  
This project follows a **five-step fine-tuning process** using **PEFT and the Unsloth library** for optimized training efficiency:  

1. **Install the dependencies.**  
2. **Load the DeepSeek-R1-Distill-Qwen-14B model.**  
3. **Prepare the medical dataset for supervised fine-tuning.**  
4. **Configure PEFT for efficient training.**  
5. **Run the fine-tuning process and evaluate performance.**  

## **Objective**  
By the end of this project, the fine-tuned DeepSeek model will be:  
✅ **More specialized for medical chatbot applications.**  
✅ **Capable of delivering accurate, explainable, and trustworthy healthcare insights.**  
✅ **Optimized for performance while maintaining computational efficiency.**  
